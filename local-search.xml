<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>test</title>
    <link href="/uncategorized/test/"/>
    <url>/uncategorized/test/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[E=mc^2\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>CS274A-Language Model</title>
    <link href="/note/CS274A/CS274A/Language-Model/"/>
    <url>/note/CS274A/CS274A/Language-Model/</url>
    
    <content type="html"><![CDATA[<h1 id="language-model">Language Model</h1><p>相对于分类、生成等任务中将语言看作 <strong>词的集合或向量</strong>，语言模型中将文本作为 <strong>词的序列</strong></p><p>通过估计条件概率进行预测</p><ul><li>n-gram: 只考虑前面n个单词的影响（作为概率的条件）</li><li>RNN</li><li>Transformer：将一些上文需要关注的单词作为条件（注意力机制）</li><li>？</li></ul><h3 id="lm的特殊处理">LM的特殊处理</h3><h5 id="unknown-word">unknown word</h5><p>构建token <code>&lt;UNK&gt;</code>,</p><ol type="1"><li>构建词表</li><li>检索训练文本中不在词表中的词<ol type="1"><li>较多的词添加到词表</li><li>较少的词换成<code>&lt;UNK&gt;</code></li></ol></li><li>正常训练</li></ol><p>也可以从字符/sub word 层面构建语言模型，避免生词出现</p><h3 id="lm的评估">LM的评估</h3><p>Extrinsic: 针对下游任务，同前</p><p>Intrinsic:</p><h4 id="perplexity">Perplexity</h4><p>困惑度：LM应该对没见过的真实句子给出较高的分数</p><p>For test data <span class="math inline">\(\bar x_{1:m}\)</span> (msentences):</p><p>average log-probability per word of <span class="math inline">\(\barx_{1:m}\)</span> is <span class="math display">\[l=-\frac1M\sum_{i=1}^m\log_2p(\bar x_i)\]</span> if <span class="math inline">\(M=\sum_{i=1}^m|\barx_i|\)</span></p><p>perplexity= <span class="math inline">\(2^l\)</span></p><blockquote><p>在信息论中表达了编码所需的最小bit数</p></blockquote><p>即希望用尽量少的词得到概率尽量大的一系列词（来对信息进行编码）</p><p>例</p><ul><li>如果训练数据概率为1，困惑度=1</li><li>如果=0，困惑度=<span class="math inline">\(\infty\)</span></li></ul><p>==注意==: 困惑度受词表影响</p><hr /><h2 id="n-gram-model">N-gram Model</h2><p>如果n=1，为Unigram model，即每个单词相互独立</p><p>用MLE，概率=序列出现次数/序列前n出现次数</p><p>问题：数据稀疏</p><p>利用平滑</p><h4 id="back-off">Back-off</h4><p>如果当前n-gram的效果不好，那么就用n+1gram</p><h4 id="interpolation">Interpolation</h4><p>线性插值</p><p>将不同n值的n-gram分别估算概率然后加权平均</p><h3 id="生成式">生成式</h3><p>Auto-aggressive generate</p><p>语法通顺但语义不行(incoherent)。</p><blockquote><p>fixed-window neural LM</p><p>n-gram的神经网络实现：将词向量连接后过线性变换和激活函数</p><p>用神经网络记住参数，不用对所有组合记录概率</p><p>同时由于同类词语可能具有类似的embedding，泛化能力更好（神经网络是平滑的，类似的输入带来类似的输出）</p></blockquote><h2 id="rnn">RNN</h2><p>每一个时刻的新隐向量都根据上一个时刻的隐向量和新的wordembedding得到</p><h4 id="学习">学习</h4><p>MLE</p><p>计算真实分布<span class="math inline">\(y^{(t)}\)</span>和 <spanclass="math inline">\(\hat y^{(t)}\)</span>的cross entropy作为loss（真实分布是one-hot的）</p><p>等同于最大化所有位置正确单词的概率</p><p><strong>问题</strong>：梯度消失/梯度爆炸</p><p>梯度爆炸导致更新步长过大；解决方案：gradientclipping：梯度大于阈值时设置为上限值</p>]]></content>
    
    
    <categories>
      
      <category>note</category>
      
      <category>CS274A</category>
      
    </categories>
    
    
    <tags>
      
      <tag>note NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>潜水员戴夫，堂堂完结！</title>
    <link href="/uncategorized/dave-the-diver/"/>
    <url>/uncategorized/dave-the-diver/</url>
    
    <content type="html"><![CDATA[<h1 id="潜水员戴夫堂堂完结">潜水员戴夫，堂堂完结！</h1><p><img src="20240313215442_1.jpg" alt="完结撒花" />感觉好久没有认真的通关一款游戏了，所以最近花了25.6h把他通关了（绝对不是因为太上头）。整体而言游玩体验不错，<del>除了后期操作重复赶路慢还有各种有病的拖时间操作以外</del>，</p><p>画风清新正反馈很好开店也很爽（妹子也很好看）还有各种吃的，总之就是把好东西都给你了，开发商能有什么坏心思呢？他只是想让你玩得高兴（还有赚钱）而已。</p><p>反正流程不长，玩了不亏<del>更何况我是白嫖的</del>。</p><p><em>P.S.最后的小彩蛋还挺有意思的，就像寿司里的青芥末激发出了游戏通关的喜悦</em><img src="20240313213504_1.jpg" alt="我在海底打车万.jpg" /></p><p><em>P.P.S今天刚在群里看完东方的meme，晚上就打上了，你有什么头猪吗</em></p><blockquote><p><strong>碎碎念</strong>希望这款游戏能开个好头，恢复花时间体会好作品的兴趣和能力作为新博客发的第一篇帖，还挺有纪念意义的w之后有时间的话希望能继续更游戏、gal、番、书的完结贴吧，咕 以上</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>game</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/uncategorized/hello-world/"/>
    <url>/uncategorized/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
  
  
  <entry>
    <title>about</title>
    <link href="/"/>
    <url>/</url>
    
    <content type="html"><![CDATA[]]></content>
    
  </entry>
  
  
  
</search>
